# -*- coding: utf-8 -*-
"""KoronaTwitter.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Yvwzf81avDclwssD-KBNkkCnXg_MV24-

1. Import potrzebnych bibliotek
"""

import numpy as np
import os
from os import path 
import pandas as pd
import tweepy
import matplotlib.pyplot as plt
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
from PIL import Image
from IPython.display import Image as im
import re
import seaborn as sns
import itertools
import nltk
from nltk import bigrams
from nltk.corpus import stopwords
import collections
import networkx as nx
import warnings
warnings.filterwarnings("ignore")

sns.set(font_scale=1.5)
sns.set_style("whitegrid")

"""2. Zdefiniowanie kluczy do połączenia się z kontem TwitterAPI"""

consumer_key = input("Wprowadź API key: ")
consumer_secret = input("Wprowadź API secret key: ")
access_token = input("Wprowadź Access token: ")
access_token_secret = input("Wprowadź Access token secret: ")

auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_token, access_token_secret)
api = tweepy.API(auth, wait_on_rate_limit=True)

"""3. Zadeklarowanie szukanych zmiennych - wyrazu "coronavirus", ilości tweetów i ich dat"""

search_words = "coronavirus"
number_of_tweets = int(1000)
date_since = "2019-03-29"

# filtrowanie by pozbyć się retweetów, które zduplikują wynik
new_search = search_words + " -filter:retweets"

"""4. Zebranie danych"""

tweets = tweepy.Cursor(api.search, q=new_search, lang="en", since=date_since).items(number_of_tweets)

"""5. Preprocessing danych"""

#zdefiniowanie funkcji usuwającej url z tweetów
def remove_url(txt):
    """Replace URLs found in a text string with nothing 
    (i.e. it will remove the URL from the string).

    Parameters
    ----------
    txt : string
        A text string that you want to parse and remove urls.

    Returns
    -------
    The same txt string with url's removed.
    """

    return " ".join(re.sub("([^0-9A-Za-z \t])|(\w+:\/\/\S+)", "", txt).split())

# wyświetlenie elementów zawartych w iteratorze jako listy tweetów bez linków url 
all_tweets= [remove_url(tweet) for tweet in [tweet.text for tweet in tweets]]

#tokenizacja i sprowadzenie wszystkich słów do małych liter
separate_lowercased_words_in_tweets = [tweet.lower().split() for tweet in all_tweets]

all_tweets = list(itertools.chain(*tweets_nsw))

#zliczenie wystąpień poszczególnych słów 
count_words = collections.Counter(all_tweets)
count_words.most_common(100)

#usunięcie z tweetów stop words
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

#usuwanie stopwords z tweetów
tweets_nsw = [[word for word in tweet_words if not word in stop_words]
              for tweet_words in separate_lowercased_words_in_tweets]

"""6. Tabele, wykresy"""

#tabelka wizualizująca najczęstsze słowa 
count_words_table = pd.DataFrame(count_words.most_common(100), columns=['wyrazy', 'liczba'])
count_words_table.head()

#graf horyzontalny przedstawiający 100 najczęstszych wyrazów 
fig, ax = plt.subplots(figsize=(20, 30))
count_words_table.sort_values(by= 'liczba').plot.barh(x='wyrazy',y='liczba', ax=ax, color="pink")
ax.set_title("Najczęściej pojawiające się wyrazy w anglojęzycznych tweetach związanych z koronawirusem")
plt.show()

#wygenerowanie chmury wyrazów, tzw. "wordcloud"
text = str(count_words)
wordcloud = WordCloud(max_font_size=50, max_words=100, background_color="white").generate(text)
plt.figure()
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()

#usunięcie z wyliczeń szukanego słowa 
collection_words = ['koronawirus']
tweets_nsw_nc = [[w for w in word if not w in collection_words]
                 for word in tweets_nsw]

# zliczenie słów z pominięciem szukanego wyrazu
all_words_nsw_nc = list(itertools.chain(*tweets_nsw_nc))
counts_nsw_nc = collections.Counter(all_words_nsw_nc)
counts_nsw_nc.most_common(30)

"""7. Bigramy"""

# Próba wizualizacji współistniejących słów z wcześniej szukanym wyrazem 'koronawirus'

terms_bigram = [list(bigrams(tweet)) for tweet in tweets_nsw_nc]
bigrams = list(itertools.chain(*terms_bigram))

# zliczenie współistniejących słów
bigram_counts = collections.Counter(bigrams)

bigram_counts.most_common(30)

#tabela
bigram_df = pd.DataFrame(bigram_counts.most_common(20),
                             columns=['bigram', 'count'])

bigram_df

# stworzenie wykresu przy pomocy biblioteki networkx
d = bigram_df.set_index('bigram').T.to_dict('records')
G = nx.Graph()

# stworzenie połączeń między węzłami
for k, v in d[0].items():
    G.add_edge(k[0], k[1], weight=(v * 10))

G.add_node("coronavirus", weight=100)
fig, ax = plt.subplots(figsize=(15, 15))

pos = nx.spring_layout(G, k=1)

nx.draw_networkx(G, pos,
                 font_size=16,
                 width=3,
                 edge_color='grey',
                 node_color='grey',
                 with_labels = False,
                 ax=ax)

for key, value in pos.items():
    x, y = value[0]+.135, value[1]+.045
    ax.text(x, y,
            s=key,
            bbox=dict(facecolor='pink', alpha=0.5),
            horizontalalignment='center', fontsize=13)
    
plt.show()